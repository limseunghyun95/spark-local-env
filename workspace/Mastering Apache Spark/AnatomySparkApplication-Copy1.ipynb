{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac2daf4-800d-4255-b3f7-320b7a818296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c7d11b-aede-4143-9229-0376dfd5244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: date\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/30 05:01:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/30 05:01:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/10/30 05:01:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('AnatomySpark2')\n",
    "    .master('spark://spark-master:7077')\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"date\", \"2025-10-30\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c506e48-66a8-41da-a015-98dac625614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.eventLog.enabled', 'true'),\n",
      " ('spark.executor.memory', '2g'),\n",
      " ('spark.driver.extraJavaOptions',\n",
      "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions '\n",
      "  '--add-opens=java.base/java.lang=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.io=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.net=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.nio=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.security.action=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED '\n",
      "  '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED '\n",
      "  '-Djdk.reflect.useDirectMethodHandle=false'),\n",
      " ('spark.app.id', 'app-20251030050105-0009'),\n",
      " ('spark.app.startTime', '1761800464309'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.executor.cores', '2'),\n",
      " ('spark.sql.warehouse.dir', '/opt/spark/spark-warehouse'),\n",
      " ('spark.app.submitTime', '1761800464121'),\n",
      " ('date', '2025-10-30'),\n",
      " ('spark.eventLog.dir', 'file:///opt/spark/events'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.history.fs.logDirectory', 'file:///opt/spark/events'),\n",
      " ('spark.master', 'spark://spark-master:7077'),\n",
      " ('spark.driver.port', '33777'),\n",
      " ('spark.driver.memory', '2g'),\n",
      " ('spark.app.name', 'AnatomySpark2'),\n",
      " ('spark.executor.extraJavaOptions',\n",
      "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions '\n",
      "  '--add-opens=java.base/java.lang=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.io=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.net=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.nio=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.security.action=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED '\n",
      "  '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED '\n",
      "  '-Djdk.reflect.useDirectMethodHandle=false'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.host', '2d418f362b0e'),\n",
      " ('spark.sql.shuffle.partitions', '8'),\n",
      " ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d73c3c9-1007-4b25-bc0e-dfe27d376d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/30 07:34:58 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/10/30 07:34:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b979c378-beac-461a-8efb-ccb35e5e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
