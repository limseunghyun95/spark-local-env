{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac2daf4-800d-4255-b3f7-320b7a818296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6457d57-a75f-4688-b7c3-e805e4048829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkConf 객체 생성\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"AnatomySpark\")\n",
    "    .setMaster(\"spark://spark-master:7077\")\n",
    "    .set(\"spark.driver.memory\", \"2g\")\n",
    "    .set(\"spark.executor.memory\", \"2g\")\n",
    "    .set(\"spark.executor.cores\", \"2\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .set(\"date\", \"2025-10-30\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c506e48-66a8-41da-a015-98dac625614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: date\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/30 04:53:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/30 04:53:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.eventLog.enabled', 'true'),\n",
      " ('spark.executor.memory', '2g'),\n",
      " ('spark.app.name', 'AnatomySpark'),\n",
      " ('spark.driver.extraJavaOptions',\n",
      "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions '\n",
      "  '--add-opens=java.base/java.lang=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.io=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.net=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.nio=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.security.action=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED '\n",
      "  '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED '\n",
      "  '-Djdk.reflect.useDirectMethodHandle=false'),\n",
      " ('spark.app.submitTime', '1761800000824'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.executor.cores', '2'),\n",
      " ('spark.sql.warehouse.dir', '/opt/spark/spark-warehouse'),\n",
      " ('spark.app.startTime', '1761800000917'),\n",
      " ('date', '2025-10-30'),\n",
      " ('spark.app.id', 'app-20251030045321-0008'),\n",
      " ('spark.eventLog.dir', 'file:///opt/spark/events'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.history.fs.logDirectory', 'file:///opt/spark/events'),\n",
      " ('spark.master', 'spark://spark-master:7077'),\n",
      " ('spark.executor.extraJavaOptions',\n",
      "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions '\n",
      "  '--add-opens=java.base/java.lang=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.invoke=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.lang.reflect=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.io=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.net=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.nio=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.ch=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.nio.cs=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.security.action=ALL-UNNAMED '\n",
      "  '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED '\n",
      "  '--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED '\n",
      "  '-Djdk.reflect.useDirectMethodHandle=false'),\n",
      " ('spark.driver.memory', '2g'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.host', '2d418f362b0e'),\n",
      " ('spark.sql.shuffle.partitions', '8'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.driver.port', '42397')]\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 초기화\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)  # SparkConf 객체 전달\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "# 확인\n",
    "pprint(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d73c3c9-1007-4b25-bc0e-dfe27d376d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-30\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8740e3-b2f0-4097-bd13-16c0da995d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.name', 'AnatomySpark'), ('spark.master', 'spark://spark-master:7077'), ('spark.driver.memory', '2g'), ('spark.executor.memory', '2g'), ('spark.executor.cores', '2'), ('spark.sql.shuffle.partitions', '8'), ('date', '2025-10-30')]\n"
     ]
    }
   ],
   "source": [
    "print(conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc82f275-5315-4549-bd81-355f873d31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.parallelize(range(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd8789fc-ea45-4310-bd77-a3969ee9bdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0xffff8ebdd940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.broadcast('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b979c378-beac-461a-8efb-ccb35e5e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
